{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An implementation of a Latent Factor Model for Amazon Ratings Prediction\n",
    "\n",
    "While not directly related to biostatistics, the purpose of this notebook is to provide a demonstration of my level of programming competence, as well as show that I have some experience in implementing algorithms from scratch. Since my coursework represents a measure of my mathematical abilities, I wanted to supplement my application by providing a measure of my programming abilities.\n",
    "\n",
    "#### A description of the problem and the algorithm\n",
    "The objective and algorithm come from a graduate level computer science course I am enrolled in at UC San Diego, CSE 258: Recommender Systems and Web Mining, from Fall 2018. Our class was tasked to find an algorithm to predict Amazon review ratings from the features within the reviews themselves, such as the text corprus, and the average ratings by users and items. While multiple algorithms were introduced for this purpose, the implementation here is called a Latent Factor Model, in which \"latent factors\" are randomly generated, then iteratively solved for using a procedure called Alternating Least Squares. I provide a high level overview, though an excellent overfiew of the technique can be found here: https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "\n",
    "The algorithm recieved considerable attention after a version of it was used in order to solve the famous Netflix Problem, as described in the preceeding document. A simplified version of the algorithm is applied here, to predict the rating in an Amazon product review.\n",
    "\n",
    "#### The data\n",
    "Each data point is a web-scraped product review from Amazon. It consists of a text corprus that makes up the review itself, and some additional features, such as item category and time of review posting. Most importantly, it contains a `ReviewerID` and an `ItemID`, and the `rating` given by a user, to the item, in their review. A head of the dataframe can be seen below.\n",
    "\n",
    "\n",
    "\n",
    "### The objective\n",
    "The task is to predict the rating given by a particular user to a particular item, in essence, trying to predict whether a user would like an item, for the greater purpose of recommending that item to the user. Naturally, this has nothing to do with biostatistics, but since the topic of data modeling and prediction is important, I feel that this demonstration is pertinent.\n",
    "\n",
    "Let $r_{u,i}$ denote the rating given to item $i$ by user $u$. Let $N$ be the number of examples, and $Train$ indicate the set of user, item pairs within the dataset.\n",
    "\n",
    "The objective function is to minimize the Mean Squared Error, given by $$\\frac{1}{N}\\sum_{i, u \\in Train} (r_{u,i} - \\hat{r}_{u,i})^2$$\n",
    "\n",
    "where $\\hat{r}_{u,i}$ is the predicted rating user $u$ gives to item $i$.\n",
    "\n",
    "### The model\n",
    "\n",
    "One simple approach to this problem is to model the rating as a function of the global average rating, the average rating given by a user $u$ and the average rating given to item $i$. This model takes on the form\n",
    "$$ r_{u, i} = \\alpha + \\beta_i + \\beta_u$$\n",
    "where $\\alpha$ is the global average rating, $\\beta_i$ is the difference in the average rating conferred to item $i$ from the global average $\\alpha$. $\\beta_u$ is similar to $\\beta_i$, but for users.\n",
    "\n",
    "While this model can achieve surprisingly good performance, it can be crude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import uniform\n",
    "from random import randint\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openingdata(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "data = list(openingdata('train.json'))\n",
    "df = pd.io.json.json_normalize(data)\n",
    "\n",
    "test = pd.read_csv('pairs_Rating.txt')\n",
    "test['reviewerID'], test['itemID'] = test['reviewerID-itemID'].str.split('-').str\n",
    "test['reviewerID'] = test.reviewerID.str.strip()\n",
    "test['itemID'] = test.itemID.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "joined_data = df.append(test, ignore_index=True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>categoryID</th>\n",
       "      <th>helpful.nHelpful</th>\n",
       "      <th>helpful.outOf</th>\n",
       "      <th>itemID</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women], [Clothing...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I402344648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>R798569390</td>\n",
       "      <td>The model in this picture has them rolled up a...</td>\n",
       "      <td>09 26, 2013</td>\n",
       "      <td>U490934656</td>\n",
       "      <td>High Waisted</td>\n",
       "      <td>1380153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Clothing, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I697650540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>R436443063</td>\n",
       "      <td>I love the look of this bra, it is what I want...</td>\n",
       "      <td>02 7, 2013</td>\n",
       "      <td>U714157797</td>\n",
       "      <td>Beautiful but size runs small</td>\n",
       "      <td>1360195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Wedding Party Gif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I464613034</td>\n",
       "      <td>19.99</td>\n",
       "      <td>5.0</td>\n",
       "      <td>R103439446</td>\n",
       "      <td>I am not comfortable with wearing my wedding b...</td>\n",
       "      <td>03 16, 2014</td>\n",
       "      <td>U507366950</td>\n",
       "      <td>Great Alternative for Nurses</td>\n",
       "      <td>1394928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Clothing, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I559560885</td>\n",
       "      <td>18.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>R486351639</td>\n",
       "      <td>Like the look of this top and really looks cut...</td>\n",
       "      <td>03 10, 2014</td>\n",
       "      <td>U307862152</td>\n",
       "      <td>One size fits all...Questionable</td>\n",
       "      <td>1394409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[Clothing, Shoes &amp; Jewelry, Women, Plus-Size,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I476005312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>R508664275</td>\n",
       "      <td>I'm quite small and the XS fits me like a regu...</td>\n",
       "      <td>07 30, 2013</td>\n",
       "      <td>U742726598</td>\n",
       "      <td>Great shirt</td>\n",
       "      <td>1375142400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          categories  categoryID  \\\n",
       "0  [[Clothing, Shoes & Jewelry, Women], [Clothing...           0   \n",
       "1  [[Clothing, Shoes & Jewelry, Women, Clothing, ...           0   \n",
       "2  [[Clothing, Shoes & Jewelry, Wedding Party Gif...           0   \n",
       "3  [[Clothing, Shoes & Jewelry, Women, Clothing, ...           0   \n",
       "4  [[Clothing, Shoes & Jewelry, Women, Plus-Size,...           0   \n",
       "\n",
       "   helpful.nHelpful  helpful.outOf      itemID  price  rating  reviewHash  \\\n",
       "0                 0              0  I402344648    NaN     4.0  R798569390   \n",
       "1                 0              0  I697650540    NaN     4.0  R436443063   \n",
       "2                 0              0  I464613034  19.99     5.0  R103439446   \n",
       "3                 0              0  I559560885  18.99     2.0  R486351639   \n",
       "4                 1              1  I476005312    NaN     5.0  R508664275   \n",
       "\n",
       "                                          reviewText   reviewTime  reviewerID  \\\n",
       "0  The model in this picture has them rolled up a...  09 26, 2013  U490934656   \n",
       "1  I love the look of this bra, it is what I want...   02 7, 2013  U714157797   \n",
       "2  I am not comfortable with wearing my wedding b...  03 16, 2014  U507366950   \n",
       "3  Like the look of this top and really looks cut...  03 10, 2014  U307862152   \n",
       "4  I'm quite small and the XS fits me like a regu...  07 30, 2013  U742726598   \n",
       "\n",
       "                            summary  unixReviewTime  \n",
       "0                      High Waisted      1380153600  \n",
       "1     Beautiful but size runs small      1360195200  \n",
       "2      Great Alternative for Nurses      1394928000  \n",
       "3  One size fits all...Questionable      1394409600  \n",
       "4                       Great shirt      1375142400  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acros\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\acros\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "users = label_enc.fit_transform(joined_data.reviewerID).reshape(-1,1)\n",
    "one_hot = OneHotEncoder(sparse = True)\n",
    "users = one_hot.fit_transform(users)\n",
    "\n",
    "training_users = users[0:200000]\n",
    "test_users = users[200000:214000]\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "items = label_enc.fit_transform(joined_data.itemID).reshape(-1,1)\n",
    "one_hot = OneHotEncoder(sparse = True)\n",
    "items = one_hot.fit_transform(items)\n",
    "\n",
    "training_items = items[0:200000]\n",
    "test_items = items[200000:214000]\n",
    "\n",
    "\n",
    "training_labels = np.asarray(joined_data[0:200000].rating)\n",
    "\n",
    "user_ratings = np.asarray(training_users.T * training_labels)\n",
    "item_ratings = np.asarray(training_items.T * training_labels)\n",
    "I_u = np.asarray(training_users.T.sum(axis = 1))\n",
    "U_i = np.asarray(training_items.T.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #join uses and items into sparse matrix\n",
    "        X = hstack((training_users, training_items))\n",
    "        #define cut points and reshape labels if not already array\n",
    "        user_cut = training_users.shape[1]\n",
    "        end_cut = X.shape[1]\n",
    "        y = np.asarray(training_labels).reshape(-1,1)\n",
    "        \n",
    "        #split into train and validation sets\n",
    "        X_train, X_val, training_labels, y_validation = train_test_split(X, y, test_size = 0.30)\n",
    "        \n",
    "        #split into validation users and validation items\n",
    "        training_users = X_train.T[0:user_cut].T\n",
    "        training_items = X_train.T[user_cut:end_cut].T\n",
    "        validation_users = X_val.T[0:user_cut].T\n",
    "        validation_items = X_val.T[user_cut:end_cut].T\n",
    "\n",
    "        y_training = training_labels.reshape(-1, 1)\n",
    "        #y_train = training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel:\n",
    "    def __init__(self, C = 1, Cx = 1, Cy = 1, K = 20, random_state = 0):\n",
    "        \"\"\"Fit a Latent Factor Model for Rating Prediction\n",
    "        \n",
    "        Accepts sparse, one-hot_encoded matrices of users and items with the \n",
    "        same number N of training examples, respectively.\n",
    "        \n",
    "        Uses Gradient Descent in order to update parameters for the intercept,\n",
    "        and the bias parameters for both users and items. Latent variable \n",
    "        approximation not yet implementented.\n",
    "        \n",
    "        Automatically divides the training data into a validation set of chosen\n",
    "        size and computes gradient descent by checking the validation loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.Cx = Cx\n",
    "        self.Cy = Cy\n",
    "        self.K = K\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _preprocess_data(self, users, items, y, val_size = 0.20):\n",
    "        \"\"\"Uses the passed users, items matrices and labels in order to generate\n",
    "        training and validation sets for model fitting and training. Users and \n",
    "        items should be sparse matrices with same number of training examples.\n",
    "        \n",
    "        val_size should be a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #join uses and items into sparse matrix\n",
    "        X = hstack((users, items))\n",
    "        #define cut points and reshape labels if not already array\n",
    "        user_cut = users.shape[1]\n",
    "        end_cut = X.shape[1]\n",
    "        y = np.asarray(y).reshape(-1,1)\n",
    "        \n",
    "        #split into train and validation sets\n",
    "        X_train, X_val, training_labels, y_val = train_test_split(X, y, test_size = val_size)\n",
    "        \n",
    "        #split into validation users and validation items\n",
    "        train_users = X_train.T[0:user_cut].T\n",
    "        train_items = X_train.T[user_cut:end_cut].T\n",
    "        val_users = X_val.T[0:user_cut].T\n",
    "        val_items = X_val.T[user_cut:end_cut].T\n",
    "\n",
    "        training_labels = training_labels.reshape(-1, 1)\n",
    "        y_train = training_labels\n",
    "        \n",
    "        #create matrices for gradient descent\n",
    "        user_ratings = np.asarray(train_users.T * y_train)\n",
    "        item_ratings = np.asarray(train_items.T * y_train)\n",
    "        user_ratings = user_ratings.reshape((-1,1))\n",
    "        item_ratings = item_ratings.reshape((-1,1))\n",
    "        I_u = np.asarray(train_users.T.sum(axis = 1))\n",
    "        U_i = np.asarray(train_items.T.sum(axis = 1))\n",
    "        \n",
    "        #store created objects\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.train_users = train_users\n",
    "        self.train_items = train_items\n",
    "        self.val_users = val_users\n",
    "        self.val_items = val_items\n",
    "        self.user_ratings = user_ratings\n",
    "        self.item_ratings = item_ratings\n",
    "        self.I_u = I_u\n",
    "        self.U_i = U_i\n",
    "    \n",
    "    def _mse_loss(self, labels, predictions ):\n",
    "        \"\"\"Calculates the mean-squared-error loss with current parameters.\"\"\"\n",
    "        \n",
    "        #make predictions with parameters and passed data\n",
    "        predictions = np.asarray(predictions)\n",
    "        #mean squared error\n",
    "        sq_err = (labels - predictions)**2\n",
    "        mean_sq_err = sq_err.mean()\n",
    "        \n",
    "        return (mean_sq_err)\n",
    "    \n",
    "    def _ALS_for_biases(self, users, items, y):\n",
    "        \n",
    "\n",
    "        #extract matrices and vectors for ALS\n",
    "        val_size = self.val_size\n",
    "        self._preprocess_data(users, items, y, val_size = val_size)\n",
    "        \n",
    "        #extract objects for optimization\n",
    "        y_train = self.y_train\n",
    "        y_val = self.y_val\n",
    "        train_users = self.train_users\n",
    "        train_items = self.train_items\n",
    "        val_users = self.val_users\n",
    "        val_items = self.val_items\n",
    "        user_ratings = self.user_ratings\n",
    "        item_ratings = self.item_ratings\n",
    "        I_u = self.I_u\n",
    "        U_i = self.U_i\n",
    "        C = self.C\n",
    "        max_iter = self.max_iter\n",
    "        tol = self.tol\n",
    "        K = self.K\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.mu_hat = 1\n",
    "        self.a_hat = np.ones((train_items.shape[1], 1))\n",
    "        self.b_hat = np.ones((train_users.shape[1], 1))\n",
    "        N = train_users.shape[0]\n",
    "        curr_iter = 0\n",
    "        \n",
    "        while True:\n",
    "            #calculate validation loss before update\n",
    "            \n",
    "            \n",
    "            y_pred_train = self.mu_hat + (train_users.dot(self.b_hat)) + (train_items.dot(self.a_hat))\n",
    "            prev_loss = self._mse_loss(y_train, y_pred_train)\n",
    "    \n",
    "            #update bias terms by iteratively solving one at a time\n",
    "            self.mu_hat =  (y_train.sum() - self.b_hat.sum() - self.a_hat.sum()) / N\n",
    "            self.b_hat =   (user_ratings - (( I_u * self.mu_hat)  + (train_users.T * (train_items * self.a_hat)))) / (C + I_u)             \n",
    "            self.a_hat =  (item_ratings - (( U_i * self.mu_hat) + (train_items.T * (train_users * self.b_hat)))) / (C + U_i)\n",
    "            \n",
    "            #calculate loss\n",
    "            y_pred_val = self.mu_hat + (val_users.dot(self.b_hat)) + (val_items.dot(self.a_hat))\n",
    "            y_pred_train = self.mu_hat + (train_users.dot(self.b_hat)) + (train_items.dot(self.a_hat))\n",
    "            curr_loss = self._mse_loss(y_train, y_pred_train)\n",
    "            \n",
    "            #completed loop\n",
    "            curr_iter += 1\n",
    "            \n",
    "            #Stopping criterion if loss stops decreasing or max iterations\n",
    "            if (abs(prev_loss - curr_loss) < tol):\n",
    "                break\n",
    "            elif (curr_iter >= 100):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "              \n",
    "        #store losses and residuals\n",
    "        residuals = y_train - y_pred_train\n",
    "        self.residuals = residuals.reshape((-1, 1))\n",
    "        self.bias_only_train_loss = self._mse_loss(y_train, y_pred_train)\n",
    "        self.bias_only_val_loss = self._mse_loss(y_val, y_pred_val)\n",
    "    \n",
    "    def _AlternatingLatentFactors(self, users, items, y):\n",
    "        \"\"\"Implements alternating least squares to find optimal parameters for latent factors\"\"\"\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Fit the bias only model\n",
    "        self._ALS_for_biases(users, items, y)\n",
    "        \n",
    "        y_train = self.y_train\n",
    "        y_val = self.y_val\n",
    "        train_users = self.train_users\n",
    "        train_items = self.train_items\n",
    "        val_users = self.val_users\n",
    "        val_items = self.val_items\n",
    "        user_ratings = self.user_ratings\n",
    "        item_ratings = self.item_ratings\n",
    "        I_u = self.I_u\n",
    "        U_i = self.U_i\n",
    "        #C = self.C\n",
    "        Cx = self.Cx\n",
    "        Cy = self.Cy\n",
    "        max_iter = self.max_iter\n",
    "        tol = self.tol\n",
    "        K = self.K\n",
    "        \n",
    "        #Randomly initalize variables as non-negative values\n",
    "        gamma_i = np.random.normal(scale = 1/K, size = (K, train_items.shape[1])).T\n",
    "        gamma_i = csr_matrix(gamma_i)\n",
    "\n",
    "        gamma_u = np.random.normal(scale = 1/K, size = (K, train_users.shape[1])).T\n",
    "        gamma_u = csr_matrix(gamma_u)\n",
    "        \n",
    "        #extract residuals to create targets for latent factors and initialize iterations\n",
    "        residual_matrix = train_users.T.dot(train_items.multiply(self.residuals))\n",
    "        curr_iter = 0\n",
    "        N = train_users.shape[0]\n",
    "        \n",
    "        #P = train_users.dot(gamma_u)\n",
    "        #Q = train_items.dot(gamma_i)\n",
    "        #gamma_dot_prod = P * Q\n",
    "        #self.gamma_dot_prod = gamma_dot_prod.sum(axis = 1).reshape((-1,1))\n",
    "    \n",
    "        #self.mu_hat = mu_hat\n",
    "        #self.a_hat = a_hat\n",
    "        #self.b_hat = b_hat\n",
    "        \n",
    "        while True:\n",
    "            #calculate validation loss before update\n",
    "            \n",
    "            if curr_iter == 0:\n",
    "                y_pred_train = self.mu_hat + (train_users.dot(self.b_hat)) + train_items.dot(self.a_hat)\n",
    "                prev_loss = self._mse_loss(y_train, y_pred_train)\n",
    "            else:\n",
    "                y_pred_train = self.predict(train_users, train_items)\n",
    "                prev_loss = self._mse_loss(y_train, y_pred_train)\n",
    "\n",
    "\n",
    "            \n",
    "            #Solve for User Latent Factors\n",
    "            YTY = gamma_i.T.dot(gamma_i)\n",
    "            lambdaI = Cy * np.eye(N = K)\n",
    "            YTY_lambdaI = YTY + lambdaI\n",
    "            YTY_lambdaI = np.asarray(YTY_lambdaI)\n",
    "            RY = residual_matrix.dot(gamma_i).toarray()\n",
    "            self.p_u = np.linalg.solve(YTY_lambdaI, RY.T)\n",
    "            #gamma_u = csr_matrix(self.p_u.T)\n",
    "            \n",
    "            #Solve for Item Latent Factors\n",
    "            XTX = gamma_u.T.dot(gamma_u)\n",
    "            lambdaI = Cx * np.eye(N = K )\n",
    "            XTX_lambdaI = XTX + lambdaI\n",
    "            XTX_lambdaI = np.asarray(XTX_lambdaI)\n",
    "            RX = residual_matrix.T.dot(gamma_u).toarray()\n",
    "            self.q_i = np.linalg.solve(XTX_lambdaI, RX.T)\n",
    "    \n",
    "            #compute dot product for training labels\n",
    "            P = train_users.dot(self.p_u.T)\n",
    "            Q = train_items.dot(self.q_i.T)\n",
    "            gamma_dot_prod = P * Q\n",
    "            self.gamma_dot_prod = gamma_dot_prod.sum(axis = 1).reshape((-1,1))\n",
    "            #y_pred = gamma_dot_prod.sum(axis = 1)\n",
    "            \n",
    "            #calculate loss\n",
    "            y_pred_val = self.predict(val_users, val_items)\n",
    "            y_pred_train = self.predict(train_users, train_items)\n",
    "            curr_loss = self._mse_loss(y_train, y_pred_train)\n",
    "            \n",
    "            curr_iter += 1\n",
    "            #print(\"Iteration: %s, train_loss: %s, val loss: %s\" % (curr_iter, \n",
    "            #                                                       round(curr_loss,5),\n",
    "            #                                                       round(self._mse_loss(y_val, y_pred_val), 5)))\n",
    "            #print(\"Train loss: %s \" % round(self._mse_loss(y_train, y_pred_train), 5))\n",
    "            #print(\"Val loss: %s\" % round(self._mse_loss(y_val, y_pred_val), 5))\n",
    "            \n",
    "            #Stopping criterion if loss stops decreasing or max iterations\n",
    "            if (curr_iter >= max_iter):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "              \n",
    "        #store the fitted parameters and training/val loss\n",
    "        #self.mu_hat = mu_hat\n",
    "        #self.b_hat = b_hat\n",
    "        #self.a_hat = a_hat\n",
    "        #self.gamma_i = gamma_i\n",
    "        #self.gamma_u = gamma_u\n",
    "        #self.p_u = p_u\n",
    "        #self.q_i = q_i\n",
    "        \n",
    "        self.train_loss = self._mse_loss(y_train, y_pred_train)\n",
    "        \n",
    "        self.validation_loss = self._mse_loss(y_val, y_pred_val)\n",
    "        self.curr_iter = curr_iter\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def fit(self,  users, items, labels, max_iter = 100, val_size = 0.20,\n",
    "           tol = 0.001):\n",
    "        \"\"\"Fit model with ALS.\"\"\"\n",
    "        \n",
    "        #extract values\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.val_size = val_size\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        self._AlternatingLatentFactors(users, items, labels)\n",
    "        \n",
    "        print(\"Finished training after %s rounds.\" % self.curr_iter)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, users, items):\n",
    "        \"\"\"Predict on a new set, users and items should have same number of encodings as\n",
    "        the training/validation data\n",
    "        \"\"\"\n",
    "        if (self.is_fitted == True):\n",
    "            #load fitted parameters\n",
    "            #mu_hat = self.mu_hat\n",
    "            #a_hat = self.a_hat\n",
    "            #b_hat = self.b_hat\n",
    "            #p_u = self.p_u\n",
    "            #q_i = self.q_i\n",
    "            #gamma_dot_prod = self.gamma_dot_prod\n",
    "            #gamma_i = self.gamma_i\n",
    "            #gamma_u = self.gamma_u\n",
    "            \n",
    "            P = users.dot(self.p_u.T)\n",
    "            Q = items.dot(self.q_i.T)\n",
    "            gamma_dot_prod = P * Q\n",
    "            gamma_dot_prod = gamma_dot_prod.sum(axis = 1).reshape((-1,1))\n",
    "        \n",
    "            predictions = self.mu_hat + users.dot(self.b_hat) + (items.dot(self.a_hat)) + gamma_dot_prod\n",
    "            predictions = np.asarray(predictions)\n",
    "            \n",
    "            return(predictions)\n",
    "        \n",
    "        else:\n",
    "            print(\"You haven't fit this model yet.\")\n",
    "            \n",
    "    def get_scores(self):\n",
    "        \"\"\"Reports training and validation loss of model.\"\"\"\n",
    "        if (self.is_fitted == True):\n",
    "            print(\"This model has a training loss of %s and a validation loss of %s.\" % \n",
    "                  (round(self.training_loss,5), round(self.validation_loss, 5)))\n",
    "        else:\n",
    "            print(\"You haven't fit this model yet.\")\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.16545 achieved with C = 5.812696455764401, Cx = 92.62435692859337, Cy = 27.69774908549177, K = 5\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.16656 achieved with C = 7.831804492178158, Cx = 39.15516052846017, Cy = 44.96934025067759, K = 10\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.1629 achieved with C = 9.356335235704393, Cx = 72.43259605688469, Cy = 44.17561177067908, K = 25\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.16414 achieved with C = 8.298013944320866, Cx = 0.16335295029090438, Cy = 38.31840537949307, K = 19\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.17762 achieved with C = 3.6352548819384345, Cx = 44.88509418940052, Cy = 77.19051143951432, K = 14\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.18136 achieved with C = 3.6907981166552735, Cx = 17.364099859531485, Cy = 80.75881065153872, K = 32\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.23649 achieved with C = 1.6394605137216118, Cx = 52.81701052468569, Cy = 37.42874380277332, K = 28\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.16734 achieved with C = 5.138779675882019, Cx = 44.8157116981239, Cy = 46.25171276296676, K = 48\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.25517 achieved with C = 1.4517671209937362, Cx = 9.877799637854471, Cy = 31.198418042097146, K = 32\n",
      "Training model...\n",
      "Finished training after 10 rounds.\n",
      "Validation Loss of 1.17747 achieved with C = 3.2258805240044324, Cx = 34.598410155260524, Cy = 96.80348020777151, K = 40\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iterations = 10\n",
    "results = []\n",
    "for k in range(iterations):\n",
    "    c = np.random.uniform(0.001, 10)\n",
    "    cx = np.random.uniform(0.001, 100)\n",
    "    cy = np.random.uniform(0.001, 100)\n",
    "    k = randint(2, 50)\n",
    "    latentmodel = LatentFactorModel(C = c, Cx = cx, Cy = cy, K = k)\n",
    "    latentmodel = latentmodel.fit(training_users, training_items, training_labels, \n",
    "                                              max_iter = 10, val_size = 0.50, tol = 0.000001)\n",
    "    score = round(latentmodel.validation_loss, 5)\n",
    "    print(\"Validation Loss of %s achieved with C = %s, Cx = %s, Cy = %s, K = %s\" % (score, c, cx, cy, k))\n",
    "    results.append((score, c, cx, cy, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are C = 9.356335235704393, Cx = 72.43259605688469, Cy = 44.17561177067908, K = 25, with a loss of 1.1629\n"
     ]
    }
   ],
   "source": [
    "best_Score, best_C, best_cx, best_cy, best_K = min(results, key=lambda item: item[0])\n",
    "print(\"The best parameters are C = %s, Cx = %s, Cy = %s, K = %s, with a loss of %s\" % \n",
    "      (best_C, best_cx, best_cy, best_K, round(best_Score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Finished training after 5 rounds.\n"
     ]
    }
   ],
   "source": [
    "latentmodel = LatentFactorModel(C = best_C, Cx = best_cx, Cy = best_cy, K = k)\n",
    "latentmodel = latentmodel.fit(training_users, training_items, y_training, \n",
    "                                              max_iter = 5, val_size = 1, tol = 0.000001)\n",
    "predictions = latentmodel.predict(validation_users, validation_items)\n",
    "predictions = np.clip(predictions, a_min = 1, a_max = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1095210615871425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_validation, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
